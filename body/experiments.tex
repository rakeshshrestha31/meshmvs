\section{Experiments}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{imgs/qualitative_evaluation.pdf}
\end{center}
\vspace{-4mm}
\caption{
    \textbf{Qualitative evaluation} on ShapeNet dataset. \textbf{From top to bottom}: one of the input images, ground truth mesh, multi-view extended Pixel2Mesh, Pixel2Mesh++, and ours.
    Our predictions are closer to the actual shape, especially for the objects with more complex topologies.}

    % Two of three input images used by all the systems are shown along with a pair of camera views of the predictions.
    % The quality of shapes generated by our method is consistently better.
    % Note that in the last input of category \emph{Watercraft} the baselines generate shapes closer to \emph{Car} while our predictions are much closer to the actual shape.
% }
\vspace{-4mm}
\label{fig:qualitative_evaluation}
\end{figure*}

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

\paragraph{Comparisons}
We evaluate the proposed method against various multi-view shape generation methods.
The state-of-the-art method is Pixel2Mesh++~\cite{wen2019pixel2mesh++} (referred as \emph{P2M++}). \cite{wen2019pixel2mesh++} also provide a baseline by directly extending Pixel2Mesh~\cite{wang2018pixel2mesh} to operate on multi-view images (referred as \emph{MVP2M}) using their statistical feature pooling method to aggregate features from multiple color images.
% This implementation is closer to \emph{Ours-P2M} system design in that it predicts the output mesh by series for GCN deformations
% starting from an initial ellipsoid without having a fairly accurate initial object mesh.
% Also, \emph{Ours-P2M} uses the same architecture for the mesh deformations as \emph{MVP2M}. Hence, the changes in performance can be directly attributed to our depth based features, added geometric constraints and attentive feature pooling.
Results from additional multi-view shape generation baselines 3D-R2N2~\cite{3dr2n2} and LSM~\cite{kar2017lsm} are also reported.
\vspace{-4mm}
\paragraph{Dataset}
We evaluate our method against the state-of-the-art methods on the dataset from~\cite{3dr2n2}
which is a subset of ShapeNet~\cite{chang2015shapenet} and has been widely used by recent 3D shape generation methods.
It contains 50K 3D CAD models from 13 categories.
Each model is rendered with a transparent background from 24 randomly chosen camera viewpoints to obtain color images.
The corresponding camera intrinsics and extrinsics are provided in the dataset.
Since the dataset does not contain depth images, we render them using a custom depth renderer at the same viewpoints as the color images and with the same camera intrinsics.
We follow the training/testing/validation split of~\cite{gkioxari2019meshrcnn}.
\vspace{-4mm}
\paragraph{Implementation}
% We train the proposed model using the same settings as Pixel2Mesh++ and MVP2M for fair comparison.
% The same 3 views from each model are used as input and the images are resized to a resolution of 224$\times$224 for feature extraction/depth prediction.
For the depth prediction module, we follow the original MVSNet~\cite{yao2018mvsnet} implementation.
The output depth dimensions reduces by a factor of 4 to 56$\times$56 from the 224$\times$224 input image.
The number of depth hypotheses is chosen as 48 which offers a balance between accuracy and running/training time efficiency.
These depth hypotheses represent values from $0.1$ m to $1.3$ m at an interval of $25$ mm.
These values were chosen based on the range of depths present in the dataset.
% The final cost volume is of dimensions 56$\times$56$\times$48 which is regularized using a 3D convolutional network.

The hierarchical features obtained from "Contrastive Depth Features Extractor" are of total 4800 dimensions for each view.
The aggregated multi-view features are compressed to 480 dimensional after applying attentive feature pooling.
5 attention heads are used for merging multi-view features.
The loss function weights are set as $\lambda_{\text{chamfer}}=1$, $\lambda_{\text{normal}}=1.6\times10^{-4}$, $\lambda_{\text{depth}}=0.1$, $\lambda_{\text{contrastive}}=0.001$ and $\lambda_{\text{voxel}}=1$.
Two settings of $\lambda_{\text{edge}}$ were used, $\lambda_{\text{edge}}=0$ (referred as \emph{Best}) which gives better quantitative results and $\lambda_{\text{edge}}=0.2$ (referred as \emph{Pretty}) which gives better qualitative results.

% Ours models are implemented in PyTorch deep learning framework using open-source Pixel2Mesh/Mesh R-CNN implementation.
\vspace{-4mm}
\paragraph{Training and Runtime}
The network is optimized using Adam optimizer with a learning rate of $10^{-4}$.
The training is done on 5 Nvidia RTX-2080 GPUs with effective batch size 5.
% The number of GPUs and batch size were chosen in order to maximize training speed while staying within the GPU memory limit.
The depth prediction network (MVSNet) is trained independently for 30 epochs.
Then the whole system is trained for another 40 epochs with the weights of the MVSNet frozen.
Our system is implemented in PyTorch deep learning framework and it takes around 60 hours for training.

% The initial learning rate is set to 1e-4 which is reduced to 1e-5 after 30 epochs and 1e-6 after 45 epochs.
% \emph{Ours-P2M} takes around 100 hours to train while \emph{Ours-MRCNN} takes around 50 hours.
% This training time is similar to Pixel2Mesh++.
\vspace{-4mm}
\paragraph{Evaluation Metric}
Following~\cite{wang2018pixel2mesh, wen2019pixel2mesh++}, we use F1-score as our evaluation metric.
The F1-score is the harmonic mean of precision and recall where the precision/recall are calculated by finding the percentage of points in the predicted/ground truth that can find a nearest neighbor from the other within a threshold.
We provide evaluations with two threshold values: $\tau$ and $2\tau$ where $\tau=10^{-4}$  m$^2$.
% (in terms of squared Euclidean distance).
% The output meshes can vary in number of vertices but are re-sampled to 6466 points before evaluating the F1-score~\cite{wang2018pixel2mesh, wen2019pixel2mesh++}. \rakesh{might not be true}
\vspace{-2mm}
\subsection{Comparison with previous Multi-view Shape Generation Methods}
We quantitatively compare our method against previous works for multi-view shape generation in~\tableref{baseline_comparison} and show the effectiveness of our methods in improving the shape quality. Our method outperforms the state-of-the-art method  Pixel2Mesh++~\cite{wen2019pixel2mesh++} with
a decrease in chamfer distance to ground truth by 34\% and 15\% increase in F1-score at threshold $\tau$.
Note that in~\tableref{baseline_comparison} the same model is trained for all the categories but accuracy on individual categories as well as average over the categories are evaluated.
We provide the chamfer distances in the appendix.
\input{tables/baseline_comparison.tex}
\vspace{-2mm}
We also provide visual results for qualitative assessment of the generated shapes by our \emph{Pretty} model in~\figref{qualitative_evaluation} which shows that it is able to more accurately predict topologically diverse shapes.

% ~\tableref{baseline_comparison} show the ablation study
% performed to quantify the improvements from different aspects of our methods.
% Our methods outperform all the baselines in all the semantic categories.
% \figref{qualitative_evaluation} shows the qualitative comparison between the shapes generated by the different methods.
% The baseline methods suffer from various artifacts and discontinuities while ours generate shapes with higher fidelity.
\vspace{-2mm}
\subsection{Ablation studies}
% Extensive ablation studies are performed to quantify the contributions of each component.
% The "stats pooling" baseline is obtained by replacing the attention-based feature pooling with statistical features from Pixel2Mesh++.
% Similarly, "simple attention" baseline is obtained by replacing our feature pooling with feature fusion technique proposed in~\cite{hu2019randla,yang2020robust} where the pooled features are weighted sum of the multi-view features.
% We can see that our rendered depth loss, contrastive depth input and attention-based feature pooling individually improve the accuracy of the predicted mesh.
\vspace{-2mm}
\paragraph{Contrastive Depth Feature Extraction}
We evaluate several methods for contrastive feature extraction (\subsecref{contrastive_depth_feature_extraction}). These methods are
1) \emph{Input Concatenation}: using the concatenated rendered and predicted depth maps as input to the VGG feature extractor,
2) \emph{Input Difference}: using the difference of the two depth maps as input to VGG,
3) \emph{Feature Concatenation}: concatenating features from rendered and predicted depths extracted by shared VGG,
4) \emph{Feature Difference}: using difference of the features from the two depth maps extracted by shared VGG, and
5) \emph{Predicted depth only}: using the VGG features from the predicted depths only.
6) \emph{Rendered depth only}: using the VGG features from the rendered depths only.
The quantitative results are summarized in Table~\ref{table:contrastive_feature_extraction} and shows that \emph{Input Concatenation} method produces better results than other formulations.
% remains more details depth information and our Contrastive Depth Feature Extraction is able to encode the difference between the predicted depth map and rendered depth map from each deformation stage.

\input{tables/contrastive_feature.tex}
\vspace{-4mm}
\paragraph{Accuracy with different settings}
\tableref{ablation_study} shows the contribution of different components towards the final accuracy. Naively extending the single-view Mesh R-CNN~\cite{gkioxari2019meshrcnn} to multiple views using statistical feature pooling~\cite{wen2019pixel2mesh++} for mesh refinement (row 1) gives an F1-score of 72.74\% for threshold $\tau$ which is 6.26\% improvement over Pixel2Mesh++.
We further extend the above method with our probabilistic multi-view voxel grid prediction in row 2 and get a 4.23\% improvement.

In row 3 of~\tableref{ablation_study} we use our contrastive depth features instead of RGB features for mesh refinement and get 2.7\% improvement.
We then replace the statistical feature pooling with the proposed attention method and get 0.19\% improvement.
The improvement is not significant on our final architecture but we found the multi-head attention to perform better on more light-weight architectures.
We also evaluate the effect of using additional regularization from contrastive depth losses: rendered depth vs predicted depth in the 5th rows of which improves the score by 0.98\%.
In row 6 we use ground truth instead of predicted depths on our final model which gives the upper bound on our mesh prediction accuracy in relation to the depth prediction accuracy as 84.58\%.

% \vspace{-4mm}
% \paragraph{Sphere initialization}
% Row 8 uses a sphere as the coarse shape instead of cubified voxel grid.

\input{tables/ablation_study.tex}

% using the difference
% Instead of using contrastive depth input i.e. concatenated predicted and rendered depths
% (refer to~\figref{system_architecture})
% we directly use a single channeled predicted depth as input. Column 4 of
% \tableref{ablation_study} and \tableref{ablation_study_mrcnn} show the results of these experiments.

% \input{tables/multiview_voxel_accuracy.tex}

% \paragraph{Different Input Modalities}
% We analyze the accuracy of using different input modalities (RGB vs Depth vs RGB-D) for the voxel branch and mesh branch (GCN refinement) in~\tableref{input_modality_accuracy}.

% \input{tables/input_modalities.tex}
\vspace{-4mm}
\paragraph{Number of View}
We test the performance of our framework with respect to the number of views.
\tableref{number_of_input_views} shows that the accuracy of our method increases as we increase the number of input views for training.
These experiments also validate that the attention-based feature pooling can efficiently encode features from different views to take advantage of larger number of views.

\tableref{number_of_test_views} shows the results when using different number of views during testing on our model trained with 3 views
which indicates that increasing the number of views during testing does not improve the accuracy while decreasing the number of views can cause a significant drop in accuracy.
% The accuracy is very close to the corresponding models trained with higher number of views indicating our model can learn with fewer number of views and generalize to a larger number.
% -> \rakesh{this conclusion is not true, training with larger number of views gets significantly better performance}
\input{tables/number_of_input_views.tex}
