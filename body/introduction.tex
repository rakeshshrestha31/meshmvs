\section{Introduction}

3D shape generation is a long-standing research problem in computer vision and computer graphics with applications in autonomous driving, augmented reality, etc. Conventional approaches mainly leverage multi-view geometry based on stereo correspondences between images but are restricted by the coverage provided by the input views. With the availability of large-scale 3D shape datasets and the success of deep learning in several computer vision tasks, 3D representations such as voxel grid~\cite{3dr2n2, tulsiani2017multi, yan2016perspective} and point cloud~\cite{yang2018foldingnet, fan2017point} have been explored for single-view 3D reconstruction.
Among them, triangle mesh representation has received the most attention as it has various desirable properties for a wide range of applications and is capable of modeling detailed geometry without high memory requirement.

Single-view 3D reconstruction methods~\cite{wang2018pixel2mesh,huang2015single,kar2015category,su2014estimating} generate the 3D shape from merely a single color image but suffer from limited visibility due to occlusion and high dependency on training views leading to weaker generalization to different input views, resulting in low quality reconstructions.
% \rakesh{Implying that multi-view methods can generalize better than single-view to different domains/semantic categories during test time might be too strong of a claim, training on cars and testing on planes might be equally bad, whether it's single-view or multi-view}
Multi-view methods~\cite{wen2019pixel2mesh++,3dr2n2,kar2017lsm,mcrecon2017} extend the input to images from different viewpoints which provides more visual information and improves the accuracy of the generated shapes.
Recent work in multi-view mesh reconstruction~\cite{wen2019pixel2mesh++} introduces a multi-view deformation network using perceptual feature from each color image for refining the meshes generated by Pixel2Mesh~\cite{wang2018pixel2mesh}.
Although promising results were obtained, this method relies on perceptual features from color images which do not explicitly encode the geometry of the objects and could restrict the accuracy of the 3D models.
The work is also constrained by the topology of the initial shape, an ellipsoid, which limits the accuracy of the shapes from subsequent refinement modules.
%Furthermore, their use of multi-view information does not take into account phenomena such as occlusion and visibility.

% Inspired by the authors~\cite{yao2020front2back} demonstrate that using intermediate, image-centric 2.5D representations instead of directly generating 3D shapes in global frame raw from 2D images can improve 3D reconstruction quality.

% But their work relies on handcrafted heuristics for view interpolation and fuse 2.5D representations to 3D shapes using traditional, non-differentiable techniques~\cite{kazhdan2013screened}.
% Additionally Pixel2Mesh~\cite{wang2018pixel2mesh} and Pixel2Mesh++~\cite{wen2019pixel2mesh++} deform a fixed template mesh to obtain the desired shapes of all the objects which restricts the topology of the final output.

In this work, we present a novel multi-view mesh generation method where we start by predicting coarse volumetric occupancy grid representations of the color images.
Using this representation for further refinement rather than a predefined template shape allows us to generate shapes with more accurate topology~\cite{gkioxari2019meshrcnn}.
We then use Graph Convolutional Network (GCN)~\cite{scarselli2008graph,wang2018pixel2mesh} to fine-tune the volumetric representation to surface mesh in a coarse-to-fine manner.
The GCN obtains features of the graph nodes (mesh vertices) from contrastive depth features alongside the RGB perceptual features.
The contrastive depth features are extracted from the rendered depth maps of the intermediate shape and predicted depth maps from a multi-view stereo network.
Constrains between the rendered depths and predicted depths at different viewpoints are further added.
This allows our system to better reason about the transformation required to deform the intermediate shapes to confirm to the predictions by the multi-view stereo network.
% We also propose an attention-based method to fuse features from multiple views that can learn the importance of different views for each of the mesh vertices allowing the system to reason about visibility and occlusion in different input views.
% \figref{system_architecture} depicts our system architecture.
% \rakesh{Note: There is no mesh upsampling between the GCNs in Mesh R-CNN based system}

Both qualitative and quantitative experimental results based on the ShapeNet~\cite{chang2015shapenet} benchmark are provided to demonstrate the effectiveness of the proposed approach. Remarkably, our method achieves the best performance among all the previous mesh generation methods, with 14\% increase in F1-score compared to the state-of-the-art.

% lead to a

% which can be easily used to extend existing single-view 3D shape generation methods to multi-view versions.
% We start by estimating the depth maps of the multi-view RGB images using an extended MVSNet~\cite{yao2018mvsnet}. This is followed by a feature extraction network for obtaining high-level features from these depth images. An attention-based feature pooling aggregates the multi-view features which are then used by graph convolutional networks to deform an initial mesh to the final shape in a coarse-to-fine manner. We use a differentiable renderer on the predicted meshes (both intermediate and final) in order to obtain additional constraints on the deformation process in terms of the rendered depths.
% The depth predictions leverage geometry information directly into the deformation process while the multi-head attention mechanism acts as an effective method to fuse the geometric features from different views facilitating the mesh generation process to produce a finer output.

