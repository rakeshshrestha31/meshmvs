\section{Introduction}

3D shape generation is a long-standing research problem in computer vision and computer graphics with applications in autonomous driving, augmented reality, etc. Conventional approaches mainly leverage multi-view geometry based on stereo correspondences between images but are restricted by the coverage provided by the input views. With the availability of large-scale 3D shape datasets and the success of deep learning in several computer vision tasks, 3D representations such as voxel grid~\cite{3dr2n2, tulsiani2017multi, yan2016perspective} and point cloud~\cite{yang2018foldingnet, fan2017point} have been explored for single-view 3D reconstruction.
Among them, triangle mesh representation has received the most attention as it has various desirable properties for a wide range of applications and is capable of modeling detailed geometry without high memory requirement.

Single-view 3D reconstruction methods~\cite{wang2018pixel2mesh,huang2015single,kar2015category,su2014estimating} generate the 3D shape from merely a single color image but suffer from occlusion and limited visibility which leads to low quality reconstructions in the unseen areas.
\siyu{What are the limitations of single-view approaches? Limited visibility, high dependency on prediction ability and weak generalization (domain transfer)... Need some discussion.}
% as well as not generalize well across different semantic categories.
Multi-view methods~\cite{wen2019pixel2mesh++,3dr2n2,kar2017lsm,mcrecon2017} extend the input to images from different viewpoints which provides more visual information and improves the accuracy of the generated shapes.
Recent work in multi-view mesh reconstruction~\cite{wen2019pixel2mesh++} introduces a multi-view deformation network using perceptual feature from each color image for refining the meshes generated by Pixel2Mesh~\cite{wang2018pixel2mesh}.
Although promising results were obtained, this method relies on perceptual features from color images which do not explicitly encode the objects' geometry and could restrict the accuracy of the 3D models.
% and could restrict the topology of the final output.
% \rakesh{the use of color images doesn't necessarily restrict the topology}
\siyu{According to the main contributions of the proposed approach, we can discuss bout the limitations of the current multi-view approaches. First, the quality of the initial mesh is weak, and the initial topology it vital to the subsequent refinement module. Second, the refinement module is only image-based, without considering the 3D information (depth) or the information of multi-view constraints. After discussing about the existed problems, we can propose our solutions.}

% Inspired by the authors~\cite{yao2020front2back} demonstrate that using intermediate, image-centric 2.5D representations instead of directly generating 3D shapes in global frame raw from 2D images can improve 3D reconstruction quality.

% But their work relies on handcrafted heuristics for view interpolation and fuse 2.5D representations to 3D shapes using traditional, non-differentiable techniques~\cite{kazhdan2013screened}.
% Additionally Pixel2Mesh~\cite{wang2018pixel2mesh} and Pixel2Mesh++~\cite{wen2019pixel2mesh++} deform a fixed template mesh to obtain the desired shapes of all the objects which restricts the topology of the final output.

% In this work, we aim to augment 3D reconstruction with 2.5D (depth) representations while still having an end-to-end deep learning system.
% This is done by using Convolutional Neural Network (CNN) features of predicted depth images for 3D reconstruction and adding geometric constrains on the predicted shapes so that they conform to the predicted depth images.
% In this way we can utilize the geometry information from intermediate 2.5D representations while still having an end-to-end trainable system.

%  Recent single-view mesh generation methods like Mesh R-CNN~\cite{gkioxari2019meshrcnn} and ~\cite{pan2019deep} deal with this problem by using coarse voxel grids as initial shapes and topology modification respectively. However, these methods use single-view color semantics to guide the mesh deformation which still restricts the mesh quality. We follow Mesh R-CNN's approach of predicting voxel grids first and refining it to obtain final triangle mesh for robustness against varying object topologies.

In this work, we present a novel multi-view mesh generation method where we start by predicting coarse volumetric occupancy grid representations for the color images of each input viewpoint independently using a shared fully convolutional network which are merged into a single voxel grid in a probabilistic fashion followed by \texttt{cubify}~\cite{gkioxari2019meshrcnn} operation to convert it to a triangle mesh. We then use Graph Convolutional Network (GCN)~\cite{scarselli2008graph,wang2018pixel2mesh} to fine-tune the cubified voxel grid in a coarse-to-fine manner. The GCN refines the coarse mesh by using the feature vector of each graph node (mesh vertices) obtained by projecting the vertices on the 2D contrastive depth features. The contrastive depth features are extracted from the rendered depth maps of the current mesh and predicted depth maps from a multi-view stereo network. We also propose an attention-based method to fuse feature from multiple views that can learn the importance of different views for each of the mesh vertices. Constrains between the intermediate refined mesh from GCN with predicted depth maps of different viewpoints further improve the final mesh quality.\siyu{The description of the proposed approaches in this paragraph is too concrete. Try to explain the intuition behind the corresponding methods and the problems they can solve.}
% \figref{system_architecture} depicts our system architecture.
% \rakesh{Note: There is no mesh upsampling between the GCNs in Mesh R-CNN based system}

By employing multi-view voxel grid generation and refining it using geometry information from both the current mesh (through the rendered depth maps) and predicted depth maps, we are able to generate high-quality meshes.\siyu{Combine the sentence above into the previous paragraph.}
We validate our method on the ShapeNet~\cite{chang2015shapenet} benchmark and our method achieves the best performance among all previous multi-view and single-view mesh generation methods.\siyu{Give some explicit statistics demonstrating the performance of your proposed approach. For example, we achieve the best performance in aaa-benchmark, and +aa\%, +bb\% performance compared with the state-of-the-art.}

% lead to a

% which can be easily used to extend existing single-view 3D shape generation methods to multi-view versions.
% We start by estimating the depth maps of the multi-view RGB images using an extended MVSNet~\cite{yao2018mvsnet}. This is followed by a feature extraction network for obtaining high-level features from these depth images. An attention-based feature pooling aggregates the multi-view features which are then used by graph convolutional networks to deform an initial mesh to the final shape in a coarse-to-fine manner. We use a differentiable renderer on the predicted meshes (both intermediate and final) in order to obtain additional constraints on the deformation process in terms of the rendered depths.
% The depth predictions leverage geometry information directly into the deformation process while the multi-head attention mechanism acts as an effective method to fuse the geometric features from different views facilitating the mesh generation process to produce a finer output.

